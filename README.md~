Neural Networks Learning Journey
This repository serves as a comprehensive learning log, documenting my progress as I follow Andrej Karpathy's "Zero to Hero" playlist. It contains Jupyter notebooks, personal handwritten notes, and practical implementations of fundamental and advanced neural network concepts.

üìå Contents
Micrograd: A Minimalistic Autograd Engine
micrograd.ipynb
Implementation of a simple yet powerful automatic differentiation engine from scratch, mimicking the core functionality of PyTorch‚Äôs autograd.
Key Concepts: Backpropagation, computational graphs, autograd implementation.
Video Reference: Building Micrograd
Makemore: Character-Level Language Modeling
makemore.ipynb
Develops a simple bigram character-level language model, setting the foundation for more sophisticated deep learning models.

Key Concepts: Bigram modeling, tokenization, torch.Tensor operations, loss functions, sampling techniques.
Video Reference: Bigram Model
makemore2.ipynb
Expands the model by implementing a multi-layer perceptron (MLP)-based character-level language model.

Key Concepts: Model training workflow, hyperparameter tuning, train/dev/test splits, handling underfitting and overfitting.
Video Reference: MLP Model
makemore3.ipynb
Examines the inner workings of multilayer perceptrons and explores essential diagnostic techniques.

Key Concepts: Activation statistics, weight initialization, batch normalization, gradient flow analysis.
Video Reference: Understanding MLP Internals
makemore4.ipynb
Manually backpropagates through a 2-layer MLP with batch normalization without using PyTorch‚Äôs autograd. Provides a deep dive into gradient flow through various network components.

Key Concepts: Manual backpropagation, cross-entropy loss, batch normalization, embeddings.
Video Reference: Manual Backpropagation
makemore5.ipynb
Extends the previous 2-layer MLP into a hierarchical, tree-like architecture, resembling a WaveNet-style convolutional neural network.

Key Concepts: Convolutional architectures, hierarchical model structures, deep learning development workflow.
Video Reference: Building a Deeper Model
GPT: Building Transformer-based Models
gpt-dev.ipynb
Implements a Generatively Pretrained Transformer (GPT) following key principles outlined in "Attention is All You Need."

Key Concepts: Transformer architecture, attention mechanisms, positional encodings, self-attention layers.
Video Reference: GPT Model
gpttokenizer.ipynb
Implements a Byte Pair Encoding (BPE) tokenizer used in large language models.

Key Concepts: Tokenization, vocabulary construction, encoding and decoding, handling text chunks efficiently.
Video Reference: Understanding Tokenization
gpt2
Attempt at reproducing the full GPT-2 model from scratch, with limitations due to hardware constraints.

Key Concepts: Large-scale model training, resource constraints, implementation challenges.
Video Reference: Reproducing GPT-2
üìñ Notes & Additional Resources
Zero_to_hero.pdf
A collection of handwritten notes summarizing key takeaways from the "Zero to Hero" series, including equations, intuitions, and insights into deep learning.
üöÄ Getting Started
To explore the notebooks, clone this repository and open the Jupyter notebooks in your preferred environment:

bash
Copy
Edit
git clone https://github.com/mayankyadav0717/neuralnetworks.git
cd neuralnetworks
jupyter notebook
ü§ù Contributions & Feedback
This repository serves as a personal learning journey, but I welcome discussions, feedback, and suggestions! Feel free to open issues or reach out.
